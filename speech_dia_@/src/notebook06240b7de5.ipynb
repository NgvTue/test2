{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING/LOADING MODEL WEIGHTS\n# THIS SHOULD MATCH THE MODEL IN LOAD_MODEL_FROM\nVER=16\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '../input/tf-longformer-v12'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '../input/tf-ver3'\n# LOAD_MODEL_FROM=None \n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '../input/tf-longformer-v12'\n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'allenai/longformer-base-4096'","metadata":{"id":"X3XDoa7_hIem","execution":{"iopub.status.busy":"2022-01-09T08:17:04.357195Z","iopub.execute_input":"2022-01-09T08:17:04.357659Z","iopub.status.idle":"2022-01-09T08:17:04.380681Z","shell.execute_reply.started":"2022-01-09T08:17:04.357575Z","shell.execute_reply":"2022-01-09T08:17:04.380072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# from transformers import *\nimport transformers\nprint('TF version',tf.__version__)\nprint('TF version',transformers.__version__)","metadata":{"id":"99RZIVAlnK_V","outputId":"69fe6606-a831-49c0-a605-bec56f67508c","execution":{"iopub.status.busy":"2022-01-09T08:17:24.252339Z","iopub.execute_input":"2022-01-09T08:17:24.252608Z","iopub.status.idle":"2022-01-09T08:17:29.173355Z","shell.execute_reply.started":"2022-01-09T08:17:24.252578Z","shell.execute_reply":"2022-01-09T08:17:29.172647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# USE MULTIPLE GPUS\n\nif os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n    strategy = tf.distribute.get_strategy()\n    print('single strategy',strategy)\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print('multiple strategy')","metadata":{"id":"J0heu_EvnRNL","outputId":"54ef7bd9-0702-4c48-eea4-c22fe33a6df0","execution":{"iopub.status.busy":"2022-01-09T08:17:29.175062Z","iopub.execute_input":"2022-01-09T08:17:29.175492Z","iopub.status.idle":"2022-01-09T08:17:29.188471Z","shell.execute_reply.started":"2022-01-09T08:17:29.175455Z","shell.execute_reply":"2022-01-09T08:17:29.18759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy","metadata":{"execution":{"iopub.status.busy":"2022-01-09T08:17:29.189649Z","iopub.execute_input":"2022-01-09T08:17:29.190062Z","iopub.status.idle":"2022-01-09T08:17:29.198524Z","shell.execute_reply.started":"2022-01-09T08:17:29.190025Z","shell.execute_reply":"2022-01-09T08:17:29.197685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n# !nvidia-smi","metadata":{"id":"P0gkKYeeo8Y5","outputId":"ffab85f6-24ca-4de4-bace-9ff1a8ebee16","execution":{"iopub.status.busy":"2022-01-09T08:17:29.200274Z","iopub.execute_input":"2022-01-09T08:17:29.200789Z","iopub.status.idle":"2022-01-09T08:17:29.206134Z","shell.execute_reply.started":"2022-01-09T08:17:29.200643Z","shell.execute_reply":"2022-01-09T08:17:29.20546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT='/kaggle/input'\nimport glob\ntrain_df = pd.read_csv(f'{INPUT}/feedback-prize-2021/train.csv')\ndef get_raw_text(x):\n    with open(f'{INPUT}/feedback-prize-2021/train/{x}.txt',\"r\") as f:\n      return f.read()\ntexts=train_df.groupby('id')['id'].first().apply(get_raw_text).reset_index(name=\"text\")\ntrain_df =pd.merge(train_df, texts, how='inner', on='id')\ndel texts \n\ntest_df = []\nfor i in glob.glob(f\"{INPUT}/feedback-prize-2021/test/*.txt\"):\n  with open(i,\"r\") as f:\n    test_df.append(\n        {'id':os.path.basename(i)[:-4],'text':f.read()}\n    )\ntest_df = pd.DataFrame(test_df)\n# Step 01 split to sentences \n\nimport glob\nimport re\nimport numpy as np\ndef word_to_sentence(value):\n  text = value + \".\"\n  sentence = []\n  for m in re.finditer(r\"((\\n\\n)|([\\.?!]))|(\\bBecause)|(\\bbecause\\b)|(\\bthe\\b)|(\\bThe\\b)|(\\bIf\\b)|(\\bif\\b)|(\\bstudents\\b)|(\\bStudents\\b)|(\\bfirst\\b)|(\\bsecond\\b)|(\\bFirst\\b)|(\\bSecond\\b)|(\\bwe\\b)|(\\bthey\\b)|(\\bWe\\b)|(\\bThey\\b)|(\\baccording\\b)|(\\bAccording\\b)\",text):\n    if len(sentence) == 0:\n      sentence.append((0,m.start(),m.end()))\n    else:\n      \n      sentence.append((sentence[-1][1],m.start(),m.end()))\n  sentence = np.array([(i[0],i[1]) for i in sentence if i[1] - i[0] > 0])\n  return sentence\n\ndef word_per_sentence(value):\n  sentence = value.sentene\n  words = []\n  for i in sentence:\n    words.append(len(value.text[i[0]:i[1]].split()))\n  return words\n\nimport multiprocessing as mp\n\nwith mp.Pool(mp.cpu_count()) as pool:\n    sentence = pool.map(word_to_sentence, train_df.groupby('id')['text'].first())\n    sentence = [{ 'id':i,'sentence':s} for i,s in zip(train_df.groupby('id')['id'].first(), sentence)]\n    sentence = pd.DataFrame(sentence)\n    train_df = pd.merge(train_df, sentence, how='inner', on='id')\n    del sentence\n    pool.close()\nab = test_df.groupby('id').first()\nabs = []\nfor i in range(ab.shape[0]):\n  # print(i)\n  sentence = word_to_sentence(ab.iloc[i].text)\n  abs.append(sentence)\n\nabs = [{ 'id':i,'sentence':s} for i,s in zip(test_df.groupby('id')['id'].first(), abs)]\nsentence = pd.DataFrame(abs)\ntest_df = pd.merge(test_df, sentence, how='inner', on='id')\ndel sentence\ndel abs\ndel ab\n","metadata":{"id":"KaryF6Tw9Hli","execution":{"iopub.status.busy":"2022-01-09T08:23:18.866001Z","iopub.execute_input":"2022-01-09T08:23:18.866548Z","iopub.status.idle":"2022-01-09T08:23:53.822531Z","shell.execute_reply.started":"2022-01-09T08:23:18.86651Z","shell.execute_reply":"2022-01-09T08:23:53.821742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_sentence(\"oadsa becau we in are\")","metadata":{"id":"N-DFDthZDWvf","outputId":"a19c8be9-497d-44d4-9318-dabd00e7dca8","execution":{"iopub.status.busy":"2022-01-09T08:26:24.867043Z","iopub.execute_input":"2022-01-09T08:26:24.86732Z","iopub.status.idle":"2022-01-09T08:26:24.876518Z","shell.execute_reply.started":"2022-01-09T08:26:24.867285Z","shell.execute_reply":"2022-01-09T08:26:24.875778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"id":"iLP2flPYDELG","outputId":"2585ebab-1ca2-4132-e716-b8f6dcff4f4b","execution":{"iopub.status.busy":"2022-01-09T08:18:54.275208Z","iopub.execute_input":"2022-01-09T08:18:54.275578Z","iopub.status.idle":"2022-01-09T08:18:54.348149Z","shell.execute_reply.started":"2022-01-09T08:18:54.275543Z","shell.execute_reply":"2022-01-09T08:18:54.347395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = train_df.sample(200)","metadata":{"id":"1ZC-wIEKEuAu","execution":{"iopub.status.busy":"2022-01-09T08:18:54.35012Z","iopub.execute_input":"2022-01-09T08:18:54.350422Z","iopub.status.idle":"2022-01-09T08:18:54.393699Z","shell.execute_reply.started":"2022-01-09T08:18:54.350388Z","shell.execute_reply":"2022-01-09T08:18:54.393004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# step 03 convert to Labels\ndef text_to_sentences_map(example):\n  \"\"\"return\n    +> example:copy value\n    //+> position_strings  = list[str] : postion_string[i] = predictionstring[i]\n    +> sentences = list[str] :  sentences[i] = sentence[i]\n    +> offset = list[tuple(int,int)] : offset[i] = (start, end) : char level sentence[i]\n  \"\"\"\n  text = example.text\n  sentence = example.sentence \n  # position_string = []\n  sentences = []\n  offsets = []\n  ss = []\n  for i in range(len(sentence)):\n    t = text[sentence[i][0] : sentence[i][1]]\n    re_punc = re.compile(\"([\\\"\\''().,;:/_?!—\\-])\")\n    t = re_punc.sub(r\" \", t)\n    if len(t.split()) >= 1:\n      # merger to next \n      ss.append(sentence[i])\n    else:\n      if i+ 1 < len(sentence):\n        sentence[i+1][0] = sentence[i][0]\n    # if sentence[i][1] - sentence[i][0] < \n  for start,end  in ss:\n    offsets.append((start, end))\n    sentences.append(text[start:end])\n  return {\n      'sentences':sentences,\n      # 'position_strings':position_string,\n      'offset':offsets,\n      'id':example.id \n  }\n\nimport tqdm\na=[]\nab = train_df.groupby('id').first().reset_index()\nfor i in tqdm.tqdm(range(ab.shape[0]),total=ab.shape[0]):\n  a.append(text_to_sentences_map(ab.iloc[i]))\ndel ab\nab =pd.DataFrame(a)\nprint(ab)\ntrain_df = pd.merge(train_df, ab, on='id', how='inner')\ntrain_df.head(1)\ndf1 = train_df.groupby('id')['discourse_start'].apply(list).reset_index(name=\"starts\")\ndf2 = train_df.groupby('id')['discourse_end'].apply(list).reset_index(name=\"ends\")\ndf3 = train_df.groupby('id')['discourse_type'].apply(list).reset_index(name=\"classes\")\ndf4 = train_df.groupby('id')['predictionstring'].apply(list).reset_index(name=\"predictionstrings\")\ntrain_df = pd.merge(train_df, df1, on='id',how='inner')\ntrain_df = pd.merge(train_df, df2, on='id',how='inner')\ntrain_df = pd.merge(train_df, df3, on='id',how='inner')\ntrain_df = pd.merge(train_df, df4, on='id',how='inner')\ntrain_df = train_df[['id','discourse_id','discourse_start','discourse_text','discourse_end','starts','ends', 'classes','sentence','sentences','offset','text','predictionstring','predictionstrings','discourse_type']]","metadata":{"id":"9U3mHtLOD9E5","outputId":"754731b7-4965-4c51-c62b-2d29150efcfc","execution":{"iopub.status.busy":"2022-01-09T08:23:53.824598Z","iopub.execute_input":"2022-01-09T08:23:53.824846Z","iopub.status.idle":"2022-01-09T08:24:07.350737Z","shell.execute_reply.started":"2022-01-09T08:23:53.824812Z","shell.execute_reply":"2022-01-09T08:24:07.350008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a=[]\nab = test_df.groupby('id').first().reset_index()\nfor i in tqdm.tqdm(range(ab.shape[0]),total=ab.shape[0]):\n  a.append(text_to_sentences_map(ab.iloc[i]))\nab =pd.DataFrame(a)\ntest_df = pd.merge(test_df, ab, on='id', how='inner')\ndel ab\ndel df1\ndel df2\ndel df3\ndel a\n","metadata":{"id":"YvX7nZQuF3hE","outputId":"853f8a04-0dbe-4436-d056-1f3e63972f87","execution":{"iopub.status.busy":"2022-01-09T08:24:07.35194Z","iopub.execute_input":"2022-01-09T08:24:07.35312Z","iopub.status.idle":"2022-01-09T08:24:07.380377Z","shell.execute_reply.started":"2022-01-09T08:24:07.353079Z","shell.execute_reply":"2022-01-09T08:24:07.37964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[20].sentences","metadata":{"id":"QuVocwrg1F_b","outputId":"6075ef2e-d09c-495f-b5cc-e3485bf10312","execution":{"iopub.status.busy":"2022-01-09T08:24:07.382158Z","iopub.execute_input":"2022-01-09T08:24:07.382563Z","iopub.status.idle":"2022-01-09T08:24:07.390861Z","shell.execute_reply.started":"2022-01-09T08:24:07.382526Z","shell.execute_reply":"2022-01-09T08:24:07.389012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.iloc[20].text","metadata":{"id":"qcoxYDCk1Mwz","outputId":"83423c1f-79ca-4b11-a616-bc679fd4db8a","execution":{"iopub.status.busy":"2022-01-09T08:24:17.668442Z","iopub.execute_input":"2022-01-09T08:24:17.668696Z","iopub.status.idle":"2022-01-09T08:24:17.674463Z","shell.execute_reply.started":"2022-01-09T08:24:17.668668Z","shell.execute_reply":"2022-01-09T08:24:17.673749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ids = train_df['id'].unique()[:100]\n# index = train_df['id'].apply(lambda x:x in ids)\n# train_df = train_df[index]","metadata":{"id":"w_Am6MARQA8S","execution":{"iopub.status.busy":"2022-01-09T08:18:54.907718Z","iopub.execute_input":"2022-01-09T08:18:54.9081Z","iopub.status.idle":"2022-01-09T08:18:54.913771Z","shell.execute_reply.started":"2022-01-09T08:18:54.908064Z","shell.execute_reply":"2022-01-09T08:18:54.913135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train_df.copy()\n# train_df = train_df.groupby('id').first().reset_index()","metadata":{"id":"54ZIJtbXFU5B","execution":{"iopub.status.busy":"2022-01-09T08:18:54.917569Z","iopub.execute_input":"2022-01-09T08:18:54.917759Z","iopub.status.idle":"2022-01-09T08:18:54.921556Z","shell.execute_reply.started":"2022-01-09T08:18:54.917738Z","shell.execute_reply":"2022-01-09T08:18:54.920864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train","metadata":{"id":"Us7bVmZ8STW-","outputId":"87a7005b-7fcb-452c-cebe-f3269955c751","execution":{"iopub.status.busy":"2022-01-04T07:55:57.615616Z","iopub.execute_input":"2022-01-04T07:55:57.615836Z","iopub.status.idle":"2022-01-04T07:55:57.756022Z","shell.execute_reply.started":"2022-01-04T07:55:57.615808Z","shell.execute_reply":"2022-01-04T07:55:57.755113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\ndef bb_intersection_over_union2(boxA, boxB):\n\t# determine the (x, y)-coordinates of the intersection rectangle\n\txA = max(boxA[0], boxB[0])\n\t# yA = max(boxA[1], boxB[1])\n\txB = min(boxA[1], boxB[1])\n\t# yB = min(boxA[3], boxB[3])\n\t# compute the area of intersection rectangle\n\tinterArea = max(0, xB - xA + 1) \n\t# compute the area of both the prediction and ground-truth\n\t# rectangles\n\tboxAArea = (boxA[1] - boxA[0] + 1) \n\tboxBArea = (boxB[1] - boxB[0] + 1) \n\t# compute the intersection over union by taking the intersection\n\t# area and dividing it by the sum of prediction + ground-truth\n\t# areas - the interesection area\n\tiou = interArea / float(boxAArea )\n\t# return the intersection over union value\n\treturn iou\ndef finding(x):\n  sentence = x.sentence\n  discourse_start = x.discourse_start\n  discourse_end = x.discourse_end\n  iou=0\n  for x in sentence:\n    iou = max(iou,bb_intersection_over_union2(x, (discourse_start,discourse_end )) )\n    # if iou >=0.9:\n      # break\n  return iou\n# a=train_df.apply(finding, axis=1)\n\n# CODE FROM : Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score\n\ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return \" \".join(str(x) for x in output)\n\ndef refine(pred, offset):\n  target_map_rev = {0:'Lead', 2:'Position', 4:'Evidence', 6:'Claim', 8:'Concluding Statement',\n             10:'Counterclaim', 12:'Rebuttal', 14:'blank'}\n\n  pred = pred \n  # print(pred)\n  res = []\n  i=0\n  M = min(len(offset), len(pred))\n  while i < M:\n    start = pred[i]\n    prediction = []\n    if start in [0,2,4,6,8,10,12]:\n        prediction.append(i)\n        i += 1\n        if i>=M: break\n        while i < M and pred[i]==start+1:\n            prediction.append(i)\n            i += 1\n            if i>=M: break\n        # print(prediction)\n        res.append({\n            'class':target_map_rev[int(pred[prediction[0]]) ],\n            'offset':(offset[prediction[0]][0], offset[prediction[-1]][1])\n        })\n    else:\n        if start != 14:\n          start  = start  - 1\n          pred[i] -= 1\n          prediction.append(i)\n          i += 1\n          if i>=M: break\n          while i < M and pred[i]==start+1:\n              prediction.append(i)\n              i += 1\n              if i>=M: break\n          # print(prediction)\n          res.append({\n              'class':target_map_rev[int(pred[prediction[0]]) ],\n              'offset':(offset[prediction[0]][0], offset[prediction[-1]][1])\n          })\n        else:\n          i += 1\n  return res\n\n# def end_to_end_file(txt):\n\n\n# class IOUCallBack(tf.keras.callbacks.Callback):\n#   def __init__(self, model, valid_df):\n#     self.model = model\n#     self.val_df = valid_df.copy()\n#     self.instancedf = valid_df.groupby(\"id\").first().reset_index()\n#     train_sentences = self.instancedf['sentences'].values\n#     train_labels = self.instancedf['labels'].values\n\n#     val_set = tf.data.Dataset.from_generator(partial(gernerator,sentences=train_sentences, labels=train_labels ), \n#                                            output_signature=(\n#                                                {\n#                                                               'tokens':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n#                                                               'attention':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n#                                                               'sentence':tf.TensorSpec(shape=(1024,MAX_SENTENCE), dtype=tf.float32)\n#                                               },\n#                                                              tf.TensorSpec(shape=(MAX_SENTENCE), dtype=tf.int32))\n                                           \n#                                            )\n#     self.val_set=val_set.batch(\n#       2, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False,\n#       name=None\n#   ).prefetch(4)\n#   self.predictionstrings=None\n#   def on_epoch_end(self, batch, logs=None):\n#     print(\"\\n\\nevaluated validation set\")\n#     predict = self.model.predict(self.val_set)\n#     predict = np.argmax(predict,-1)\n#     print(\"predict done : \", predict.shape)\n#     print(predict)\n#     predictionstrings =[]\n#     for i in range(len(self.instancedf)):\n#       sample =self.instancedf.iloc[i]\n#       offset = sample.offset \n#       text = sample.text\n#       res = refine(predict[i], offset)\n#       for r in res:\n#         predictionstrings.append(\n#             {\n#                 \"predictionstring\":calc_word_indices(text, r['offset'][0],r['offset'][1]),\n#                 \"id\":sample.id,\n#                 \"class\":r['class']\n#             }\n\n#         )\n#     oof=pd.DataFrame(predictionstrings)\n#     f1s = []\n#     if oof.shape[0] ==0:\n#       print(\"predict none\")\n#       return\n#     CLASSES = oof['class'].unique()\n#     for c in CLASSES:\n#       pred_df = oof.loc[oof['class']==c].copy()\n#       gt_df = self.val_df.loc[self.val_df['discourse_type']==c].copy()\n#       f1 = score_feedback_comp(pred_df, gt_df)\n#       print(c,f1)\n#       f1s.append(f1)\n#     self.predictionstrings = oof\n#     print()\n#     print('Overall',np.mean(f1s))\n#     print(\"to prediction strings\\n\\n\")\n\nfrom collections import Counter\ndef bb_intersection_over_union(boxA, boxB):\n\t# determine the (x, y)-coordinates of the intersection rectangle\n\txA = max(boxA[0], boxB[0])\n\t# yA = max(boxA[1], boxB[1])\n\txB = min(boxA[1], boxB[1])\n\t# yB = min(boxA[3], boxB[3])\n\t# compute the area of intersection rectangle\n\tinterArea = max(0, xB - xA + 1) \n\t# compute the area of both the prediction and ground-truth\n\t# rectangles\n\tboxAArea = (boxA[1] - boxA[0] + 1) \n\tboxBArea = (boxB[1] - boxB[0] + 1) \n\t# compute the intersection over union by taking the intersection\n\t# area and dividing it by the sum of prediction + ground-truth\n\t# areas - the interesection area\n\tiou = interArea / float(boxAArea )\n\t# return the intersection over union value\n\treturn iou\n\n# a = [1936, 2401, 2916, 4761, 9216, 9216, 9604, 9801] \n\n# c = Counter(a)\n\n# print(c.most_common(1)) # the one most common element... 2 would mean the 2 most common\n# [(9216, 2)] # a set containing the element, and it's count in 'a'\n\ndef provide_label_sentence(example):\n  sentence = example.sentences\n  labels = []\n  z=[0 for i in example.classes]\n\n  for offset in example.offset:\n    st = offset[0]\n    end = offset[1]\n    b=[]\n    for i in range(len(example.starts)):\n      sts = example.starts[i]\n      ends = example.ends[i]\n      # if (abs(sts - st) + abs(end - ends) < 6 + 6) or (abs(sts - st) < 6 and end <= ends and end > sts) or (sts < st and\n      #                                                                                                    st < ends and abs(end-ends) < 6) or (sts<=st and end<=ends):\n      #   b.append(i)\n      iou = bb_intersection_over_union(offset, (sts,ends))\n      iou2 = bb_intersection_over_union((sts,ends),offset)\n      if  iou>= 0.5 or iou2>=0.5 :\n          b.append((i,max(iou,iou2)))\n    if len(b) > 1:\n      b = sorted(b, key=lambda x:x[1])\n#       print(example.id)\n      # cc = Counter(b)\n      # labels.append(cc.most_common(1)[0][0])\n      labels.append(b[-1][0])\n    elif len(b):\n      labels.append(b[0][0])\n    if len(b) == 0:\n      labels.append(-1)\n  refine = []\n  for i in range(len(labels)):\n    if labels[i] == -1:\n      refine.append(\"O\")\n    else:\n      l = example.classes[labels[i]]\n      if i==0 or labels[i-1] != labels[i]:\n        refine.append('B-' + l)\n      else:\n        refine.append(\"I-\" +l)\n        # print(example)\n  return refine","metadata":{"id":"XJ6gK5RpLJKp","execution":{"iopub.status.busy":"2022-01-09T08:18:54.923203Z","iopub.execute_input":"2022-01-09T08:18:54.923655Z","iopub.status.idle":"2022-01-09T08:18:54.963714Z","shell.execute_reply.started":"2022-01-09T08:18:54.923619Z","shell.execute_reply":"2022-01-09T08:18:54.963015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# provide_label_sentence(train_df.iloc[0])","metadata":{"id":"riPaUC9UPm8t","outputId":"86d3b1b9-0d67-41f9-c083-5bcc7724598a","execution":{"iopub.status.busy":"2022-01-09T08:18:54.965504Z","iopub.execute_input":"2022-01-09T08:18:54.966297Z","iopub.status.idle":"2022-01-09T08:18:54.974602Z","shell.execute_reply.started":"2022-01-09T08:18:54.966259Z","shell.execute_reply":"2022-01-09T08:18:54.973808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.iloc[0]","metadata":{"id":"r5o4NUBRPnDd","outputId":"0a7ce395-8eb5-4611-ca41-5aa8ecc08b1a","execution":{"iopub.status.busy":"2022-01-09T08:18:54.976046Z","iopub.execute_input":"2022-01-09T08:18:54.976283Z","iopub.status.idle":"2022-01-09T08:18:54.983145Z","shell.execute_reply.started":"2022-01-09T08:18:54.97625Z","shell.execute_reply":"2022-01-09T08:18:54.98233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[train['id']=='09FF4F0D2B79']\n!pip install multiprocesspandas","metadata":{"id":"u2mzhc_APwf6","execution":{"iopub.status.busy":"2022-01-09T08:18:54.984477Z","iopub.execute_input":"2022-01-09T08:18:54.984725Z","iopub.status.idle":"2022-01-09T08:19:03.704486Z","shell.execute_reply.started":"2022-01-09T08:18:54.984691Z","shell.execute_reply":"2022-01-09T08:19:03.703681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labels= []\n# for i in tqdm.tqdm(range(train_df.shape[0])):\n#   # print(i)\n#   labels.append(provide_label_sentence(train_df.iloc[i]))\ntrain = train_df.copy()\ntrain_df = train_df.groupby('id').first().reset_index()\nimport time\nfrom multiprocesspandas import applyparallel\n# train_texts, valid, oof = None, None, None\nst=time.time()\nlabels= train_df.apply_parallel(provide_label_sentence, num_processes=30, axis=0)\n# for i in tqdm.tqdm(range(train_df.shape[0])):\n#   # print(i)\n#   labels.append(provide_label_sentence(train_df.iloc[i]))\nprint(time.time() - st)","metadata":{"id":"_vRqtuAFyT1Q","outputId":"a07a4b36-6489-4097-d65b-862aeb8f3708","execution":{"iopub.status.busy":"2022-01-09T08:19:03.707838Z","iopub.execute_input":"2022-01-09T08:19:03.70855Z","iopub.status.idle":"2022-01-09T08:19:08.290463Z","shell.execute_reply.started":"2022-01-09T08:19:03.708506Z","shell.execute_reply":"2022-01-09T08:19:08.28964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"id":"Aqi_e6w0QigA","outputId":"286e178b-2ba7-415e-d74a-20e88ef87b1b","execution":{"iopub.status.busy":"2022-01-09T08:19:08.292107Z","iopub.execute_input":"2022-01-09T08:19:08.292371Z","iopub.status.idle":"2022-01-09T08:19:08.301445Z","shell.execute_reply.started":"2022-01-09T08:19:08.292325Z","shell.execute_reply":"2022-01-09T08:19:08.300714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['labels'] = labels\ndel labels","metadata":{"id":"OiWC5b3yyV6E","execution":{"iopub.status.busy":"2022-01-09T08:19:08.302811Z","iopub.execute_input":"2022-01-09T08:19:08.303467Z","iopub.status.idle":"2022-01-09T08:19:08.308992Z","shell.execute_reply.started":"2022-01-09T08:19:08.303429Z","shell.execute_reply":"2022-01-09T08:19:08.308143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df[train_df['id'] == 'ECA4D673EC46'].iloc[0])\ntrain_df.head(1)","metadata":{"id":"Hx20cxcQMBJQ","outputId":"d4dd3ce1-faf6-422c-e8eb-29652c3bf9da","execution":{"iopub.status.busy":"2022-01-09T08:19:08.311899Z","iopub.execute_input":"2022-01-09T08:19:08.313616Z","iopub.status.idle":"2022-01-09T08:19:08.384456Z","shell.execute_reply.started":"2022-01-09T08:19:08.313588Z","shell.execute_reply":"2022-01-09T08:19:08.383741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df[train_df['id'] == 'ECA4D673EC46'].iloc[0].text[162:276])","metadata":{"id":"03QwtOO8y6po","execution":{"iopub.status.busy":"2022-01-09T08:19:08.385584Z","iopub.execute_input":"2022-01-09T08:19:08.385804Z","iopub.status.idle":"2022-01-09T08:19:08.389462Z","shell.execute_reply.started":"2022-01-09T08:19:08.385773Z","shell.execute_reply":"2022-01-09T08:19:08.388794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df[train_df['id'] == 'ECA4D673EC46'].iloc[0].text[276:339])","metadata":{"id":"Mt5BOn4LMSuQ","execution":{"iopub.status.busy":"2022-01-09T08:19:08.390668Z","iopub.execute_input":"2022-01-09T08:19:08.39108Z","iopub.status.idle":"2022-01-09T08:19:08.397019Z","shell.execute_reply.started":"2022-01-09T08:19:08.391046Z","shell.execute_reply":"2022-01-09T08:19:08.396329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df[train_df['id'] == 'ECA4D673EC46'].iloc[0].offset)","metadata":{"id":"WjI-lZOCLlXI","execution":{"iopub.status.busy":"2022-01-09T08:19:08.399501Z","iopub.execute_input":"2022-01-09T08:19:08.400489Z","iopub.status.idle":"2022-01-09T08:19:08.405655Z","shell.execute_reply.started":"2022-01-09T08:19:08.400101Z","shell.execute_reply":"2022-01-09T08:19:08.405003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"id":"344PuGoW8oeg","execution":{"iopub.status.busy":"2022-01-09T08:19:08.406645Z","iopub.execute_input":"2022-01-09T08:19:08.408628Z","iopub.status.idle":"2022-01-09T08:19:11.478823Z","shell.execute_reply.started":"2022-01-09T08:19:08.408589Z","shell.execute_reply":"2022-01-09T08:19:11.478094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(MODEL_NAME)","metadata":{"id":"RcuJosCc84ws","outputId":"dee5135a-9c11-49b0-8b0f-007afe535e98","execution":{"iopub.status.busy":"2022-01-09T08:19:11.480051Z","iopub.execute_input":"2022-01-09T08:19:11.480297Z","iopub.status.idle":"2022-01-09T08:19:11.486046Z","shell.execute_reply.started":"2022-01-09T08:19:11.480264Z","shell.execute_reply":"2022-01-09T08:19:11.485404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# co cac labels tokenizer text \n# step 01 :  processing sentence \n# step 02 :  join sentence \n# step 03:   get char level index sentence \n# step 04:   tokenizer transformer \n# step 05:   count total token persentence with char level index \n# Affter step 05: input_ids = [1,1024], sentences_ids = [1024, 160] = [1,160] ","metadata":{"id":"-tusxPbfy7ew","execution":{"iopub.status.busy":"2022-01-09T08:19:11.487151Z","iopub.execute_input":"2022-01-09T08:19:11.487697Z","iopub.status.idle":"2022-01-09T08:19:11.50209Z","shell.execute_reply.started":"2022-01-09T08:19:11.487536Z","shell.execute_reply":"2022-01-09T08:19:11.501394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH  = 1024\nMAX_SENTENCE = 400\nDICT_LABEL= {'B-Lead':0,'I-Lead':1, 'B-Position':2,'I-Position':3, 'B-Evidence':4, 'I-Evidence':5, 'B-Claim':6,  'I-Claim':7,\n             'B-Concluding Statement':8,'I-Concluding Statement':9,\n             'B-Counterclaim':10, 'I-Counterclaim':11,'B-Rebuttal':12,'I-Rebuttal':13,'O':14,\"pad\":-1}\ndef clean(sent):\n  re_punc = re.compile(\"([\\\"\\''().,;:/_?!—\\-])\")\n  sent = re_punc.sub(r\" \", sent)\n  # todos: +> augment online - augmen offline: word level \n  return sent \ndef share_clean(sentences):\n  sentences = [clean(s) + \".\" for s in sentences][:MAX_SENTENCE]\n  # todos : augment sentence level \n  # todos: augment NER sentence level \n  # todos: make it offline possible to handle outline\n  char_level = []\n  for s in sentences:\n    if len(char_level)==0: \n      char_level.append(len(s))\n    else:\n      char_level.append(char_level[-1]  + len(s))\n  sentence = \"\".join(sentences)\n\n  tokens = tokenizer.encode_plus(sentence, max_length=MAX_LENGTH, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n  sentence_mask  =  [ [0.,] * 1024 for i in  range(MAX_SENTENCE)]\n  index_sent = 0\n  for index_w,o in enumerate(tokens['offset_mapping']):\n    if o == (0,0) and index_w: # padding and <s> token\n      continue \n    while o[1] > char_level[index_sent] or o[0] > char_level[index_sent]:\n      index_sent += 1\n    if o[1] <= char_level[index_sent]:\n      sentence_mask[index_sent][index_w]  = 1\n    else:\n      print(index_sent, index_w,o[1])\n  # print(index_sent)\n  sentence_mask = np.array(sentence_mask)\n  sentence_mask  = sentence_mask / (1e-8+np.sum(sentence_mask,-1)[:,None])\n  return tokens,sentence_mask.T\n","metadata":{"id":"9o_RA85_3vBk","execution":{"iopub.status.busy":"2022-01-09T08:19:11.503614Z","iopub.execute_input":"2022-01-09T08:19:11.50392Z","iopub.status.idle":"2022-01-09T08:19:11.517547Z","shell.execute_reply.started":"2022-01-09T08:19:11.503885Z","shell.execute_reply":"2022-01-09T08:19:11.516808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# share_clean(train_df.iloc[2].sentences)\n# train_df['x'] = train_df['sentences'].apply(lambda x:len(x))\n# train_df[train_df['x'] == 1]","metadata":{"id":"uMPfNVJYFGB4","execution":{"iopub.status.busy":"2022-01-09T08:19:11.519016Z","iopub.execute_input":"2022-01-09T08:19:11.519472Z","iopub.status.idle":"2022-01-09T08:19:11.527245Z","shell.execute_reply.started":"2022-01-09T08:19:11.519435Z","shell.execute_reply":"2022-01-09T08:19:11.526468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df[train_df['x'] == 1].iloc[0].text)","metadata":{"id":"Ko7CVsyFIdrc","execution":{"iopub.status.busy":"2022-01-09T08:19:11.528463Z","iopub.execute_input":"2022-01-09T08:19:11.529288Z","iopub.status.idle":"2022-01-09T08:19:11.534559Z","shell.execute_reply.started":"2022-01-09T08:19:11.529211Z","shell.execute_reply":"2022-01-09T08:19:11.533838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token, mask=share_clean(train_df.iloc[398].sentences)\n# mask.shape","metadata":{"id":"EqCHuVZfFuzQ","execution":{"iopub.status.busy":"2022-01-09T08:19:11.540614Z","iopub.execute_input":"2022-01-09T08:19:11.541192Z","iopub.status.idle":"2022-01-09T08:19:11.544732Z","shell.execute_reply.started":"2022-01-09T08:19:11.541153Z","shell.execute_reply":"2022-01-09T08:19:11.543865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_df.iloc[398])\nimport numpy as np","metadata":{"id":"yD6ZUx_1HTEc","execution":{"iopub.status.busy":"2022-01-09T08:19:11.546251Z","iopub.execute_input":"2022-01-09T08:19:11.546891Z","iopub.status.idle":"2022-01-09T08:19:11.551437Z","shell.execute_reply.started":"2022-01-09T08:19:11.546765Z","shell.execute_reply":"2022-01-09T08:19:11.550432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IDS=[1,]*16000\n# MAX_LENGTH=1024\n# MAX_SENTENCE=150\n# train_tokens = np.zeros((len(IDS),MAX_LENGTH), dtype='int32')\n# train_attention = np.zeros((len(IDS),MAX_LENGTH), dtype='int32')\n# train_mask = np.zeros((len(IDS),MAX_LENGTH, MAX_SENTENCE))","metadata":{"id":"Q_7D76VTeb1X","execution":{"iopub.status.busy":"2022-01-09T08:19:11.552822Z","iopub.execute_input":"2022-01-09T08:19:11.553223Z","iopub.status.idle":"2022-01-09T08:19:11.558698Z","shell.execute_reply.started":"2022-01-09T08:19:11.553126Z","shell.execute_reply":"2022-01-09T08:19:11.557911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  tokens = []\n#  attentions=[]\n#  sentences = []\n#  labels = []\n# from tensorflow.keras.utils.np_utils import to_categorical   \n\n\nfrom functools import partial\ndef gernerator(sentences, labels, shuffle=False):\n  if shuffle:\n    idx = np.arange(len(sentences))\n    np.random.shuffle(idx)\n  else:\n    idx = np.arange(len(sentences))\n  for index in idx:\n    sen = sentences[index]\n    l = labels[index]\n    token, train_mask=share_clean(sen)\n    input_ids=token['input_ids']\n    attention_mask = token['attention_mask']\n    label= l[:MAX_SENTENCE]\n    if len(label) < MAX_SENTENCE:\n      label.extend([\"pad\",]*(MAX_SENTENCE - len(label)))\n    labelsx = [DICT_LABEL[i] for i in label] # 1024,1\n    labelsx = np.array(labelsx).reshape(-1,)\n    # categorical_labels = to_categorical(labels, num_classes=15)\n    yield {\n        'tokens':input_ids,\n        'attention':attention_mask,\n        'sentence':train_mask\n    }, labelsx\n\n\ntrain_sentences = train_df['sentences'].values\ntrain_labels = train_df['labels'].values\n\n\nIDS = train_df.id.unique()\n# print('There are',len(IDS),'train texts.')\n# train_tokens = np.zeros((len(IDS),MAX_LENGTH), dtype='int32')\n# train_attention = np.zeros((len(IDS),MAX_LENGTH), dtype='int32')\n# # train_mask = np.zeros((len(IDS),MAX_LENGTH, MAX_SENTENCE))\n# os.makedi\n# labels =[]\n# for i in tqdm.tqdm(range(train_df.shape[0])):\n#     # print(i)\n#     token, mask=share_clean(train_df.iloc[i].sentences)\n#     # tokens.append(token['input_ids'])\n#     # attentions.append(token['attention_mask'])\n#     # sentences.append(mask)\n#     train_tokens[i,:] = token['input_ids']\n#     train_attention[i,:] = token['attention_mask']\n#     train_mask[i,:,:] = mask\n#     label= train_df.iloc[i].labels[:MAX_SENTENCE]\n#     if len(label) < MAX_SENTENCE:\n#       label.extend([\"O\",]*(MAX_SENTENCE - len(label)))\n#     labels.extend([DICT_LABEL[i] for i in label])\n# labels  = np.array(labels).reshape(train_df.shape[0],-1)","metadata":{"id":"KOSoQcbS9L1A","execution":{"iopub.status.busy":"2022-01-09T08:22:07.218093Z","iopub.execute_input":"2022-01-09T08:22:07.218631Z","iopub.status.idle":"2022-01-09T08:22:07.228389Z","shell.execute_reply.started":"2022-01-09T08:22:07.218595Z","shell.execute_reply":"2022-01-09T08:22:07.227533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"-ZR4uje-kD03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"h4OvWl0eHDXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"KlexiycilDfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n# valid_idx = train_idx\ntrain_df2 = pd.read_csv(f'{INPUT}/feedback-prize-2021/train.csv')\nnp.random.seed(None)\nprint('Train size',len(train_idx),', Valid size',len(valid_idx))","metadata":{"id":"Kc9wPsflEvw4","outputId":"8c43aba0-d708-4994-ab82-7f59629aedf3","execution":{"iopub.status.busy":"2022-01-09T08:21:39.374389Z","iopub.execute_input":"2022-01-09T08:21:39.375085Z","iopub.status.idle":"2022-01-09T08:21:40.152778Z","shell.execute_reply.started":"2022-01-09T08:21:39.37503Z","shell.execute_reply":"2022-01-09T08:21:40.152065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_labels","metadata":{"id":"_bcZwwJcmwWd","execution":{"iopub.status.busy":"2022-01-09T08:19:12.347109Z","iopub.execute_input":"2022-01-09T08:19:12.347365Z","iopub.status.idle":"2022-01-09T08:19:12.351579Z","shell.execute_reply.started":"2022-01-09T08:19:12.34732Z","shell.execute_reply":"2022-01-09T08:19:12.350796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_pd = train_df2.loc[train_df2['id'].isin(IDS[valid_idx])]","metadata":{"id":"PeVeu6PdlFxI","execution":{"iopub.status.busy":"2022-01-09T08:19:12.353078Z","iopub.execute_input":"2022-01-09T08:19:12.353354Z","iopub.status.idle":"2022-01-09T08:19:12.372822Z","shell.execute_reply.started":"2022-01-09T08:19:12.35331Z","shell.execute_reply":"2022-01-09T08:19:12.372132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_pd = pd.merge(valid_pd,train_df.iloc[valid_idx], on='id',how='left')","metadata":{"id":"vR-UP7NinW_2","execution":{"iopub.status.busy":"2022-01-09T08:19:12.374771Z","iopub.execute_input":"2022-01-09T08:19:12.37499Z","iopub.status.idle":"2022-01-09T08:19:12.386237Z","shell.execute_reply.started":"2022-01-09T08:19:12.374944Z","shell.execute_reply":"2022-01-09T08:19:12.385529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_pd['offset'] = valid_pd['offset_x']\n# valid_pd['text'] = valid_pd['text_x']\nvalid_pd['predictionstring'] = valid_pd['predictionstring_x']\n# valid_pd['sentences'] = valid_pd['sentences_y']\nvalid_pd['labels'] = valid_pd['labels']\nvalid_pd['discourse_type'] = valid_pd['discourse_type_x']","metadata":{"id":"REugDvivojD2","execution":{"iopub.status.busy":"2022-01-09T08:19:12.387705Z","iopub.execute_input":"2022-01-09T08:19:12.388353Z","iopub.status.idle":"2022-01-09T08:19:12.395126Z","shell.execute_reply.started":"2022-01-09T08:19:12.388302Z","shell.execute_reply":"2022-01-09T08:19:12.39422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_pd","metadata":{"id":"URuOg4_toUW1","outputId":"1909a34d-5bfb-4f53-f752-8ea99c3d392a","execution":{"iopub.status.busy":"2022-01-09T08:19:12.3971Z","iopub.execute_input":"2022-01-09T08:19:12.397513Z","iopub.status.idle":"2022-01-09T08:19:12.979658Z","shell.execute_reply.started":"2022-01-09T08:19:12.397477Z","shell.execute_reply":"2022-01-09T08:19:12.978866Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = tf.data.Dataset.from_generator(partial(gernerator,sentences=train_sentences[train_idx], labels=train_labels[train_idx],shuffle=True ), \n                                           output_signature=(\n                                               {\n                                                              'tokens':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n                                                              'attention':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n                                                              'sentence':tf.TensorSpec(shape=(1024,MAX_SENTENCE), dtype=tf.float32)\n                                              },\n                                                             tf.TensorSpec(shape=(MAX_SENTENCE), dtype=tf.int32))\n                                           \n                                           )\n# val_set = tf.data.Dataset.from_generator(partial(gernerator,sentences=train_sentences[valid_idx], labels=train_labels[valid_idx] ), \n#                                            output_signature=(\n#                                                {\n#                                                               'tokens':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n#                                                               'attention':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n#                                                               'sentence':tf.TensorSpec(shape=(1024,150), dtype=tf.float32)\n#                                               },\n#                                                              tf.TensorSpec(shape=(150), dtype=tf.int32))\n                                           \n#                                            )\n\n","metadata":{"id":"JYDNnfg59Sto","execution":{"iopub.status.busy":"2022-01-09T08:22:10.754796Z","iopub.execute_input":"2022-01-09T08:22:10.755351Z","iopub.status.idle":"2022-01-09T08:22:10.782259Z","shell.execute_reply.started":"2022-01-09T08:22:10.755308Z","shell.execute_reply":"2022-01-09T08:22:10.781582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_set.take(10):\n  print(i[1])","metadata":{"id":"LVj5rnd-E9_F","outputId":"ec1c3d84-0e01-44c2-efa7-ed1718cb1afe","execution":{"iopub.status.busy":"2022-01-09T08:22:10.966887Z","iopub.execute_input":"2022-01-09T08:22:10.967528Z","iopub.status.idle":"2022-01-09T08:22:11.718682Z","shell.execute_reply.started":"2022-01-09T08:22:10.967493Z","shell.execute_reply":"2022-01-09T08:22:11.717972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set=train_set.batch(\n    4, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False,\n).prefetch(8)\n# val_set=val_set.prefetch(4,).batch(\n#     2, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False,\n#     name=None\n# )","metadata":{"id":"b0_lgfSNiTYS","execution":{"iopub.status.busy":"2022-01-09T08:19:17.663692Z","iopub.execute_input":"2022-01-09T08:19:17.663977Z","iopub.status.idle":"2022-01-09T08:19:17.676262Z","shell.execute_reply.started":"2022-01-09T08:19:17.663929Z","shell.execute_reply":"2022-01-09T08:19:17.675572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dfxx","metadata":{"id":"s89Ywy2Xq5-k","execution":{"iopub.status.busy":"2022-01-09T08:19:17.677543Z","iopub.execute_input":"2022-01-09T08:19:17.678309Z","iopub.status.idle":"2022-01-09T08:19:17.683377Z","shell.execute_reply.started":"2022-01-09T08:19:17.67827Z","shell.execute_reply":"2022-01-09T08:19:17.682511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CODE FROM : Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP / (TP + 0.5*(FP+FN))\n    return my_f1_score\n\ndef calc_word_indices(full_text, discourse_start, discourse_end):\n    start_index = len(full_text[:discourse_start].split())\n    token_len = len(full_text[discourse_start:discourse_end].split())\n    output = list(range(start_index, start_index + token_len))\n    if output[-1] >= len(full_text.split()):\n        output = list(range(start_index, start_index + token_len-1))\n    return \" \".join(str(x) for x in output)\n\ndef refine(pred, offset):\n  target_map_rev = {0:'Lead', 2:'Position', 4:'Evidence', 6:'Claim', 8:'Concluding Statement',\n             10:'Counterclaim', 12:'Rebuttal', 14:'blank'}\n\n  pred = pred \n  # print(pred)\n  res = []\n  i=0\n  while i < len(offset):\n    start = pred[i]\n    prediction = []\n    if start in [0,2,4,6,8,10,12]:\n        prediction.append(i)\n        i += 1\n        if i>=len(offset): break\n        while i < len(offset) and pred[i]==start+1:\n            prediction.append(i)\n            i += 1\n            if i>=len(offset): break\n        # print(prediction)\n        res.append({\n            'class':target_map_rev[int(pred[prediction[0]]) ],\n            'offset':(offset[prediction[0]][0], offset[prediction[-1]][1])\n        })\n    else:\n        if start != 14:\n          start  = start  - 1\n          pred[i] -= 1\n          prediction.append(i)\n          i += 1\n          if i>=len(offset): break\n          while i < len(offset) and pred[i]==start+1:\n              prediction.append(i)\n              i += 1\n              if i>=len(offset): break\n          # print(prediction)\n          res.append({\n              'class':target_map_rev[int(pred[prediction[0]]) ],\n              'offset':(offset[prediction[0]][0], offset[prediction[-1]][1])\n          })\n        else:\n          i += 1\n  return res\n\n\nclass IOUCallBack(tf.keras.callbacks.Callback):\n  def __init__(self, model, valid_df):\n    self.model = model\n    self.val_df = valid_df.copy()\n    self.instancedf = valid_df.groupby(\"id\").first().reset_index()\n    train_sentences = self.instancedf['sentences'].values\n    train_labels = self.instancedf['labels'].values\n\n    val_set = tf.data.Dataset.from_generator(partial(gernerator,sentences=train_sentences, labels=train_labels ), \n                                           output_signature=(\n                                               {\n                                                              'tokens':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n                                                              'attention':tf.TensorSpec(shape=(1024), dtype=tf.int32),\n                                                              'sentence':tf.TensorSpec(shape=(1024,MAX_SENTENCE), dtype=tf.float32)\n                                              },\n                                                             tf.TensorSpec(shape=(MAX_SENTENCE), dtype=tf.int32))\n                                           \n                                           )\n    self.val_set=val_set.batch(\n      2, drop_remainder=False, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False,\n\n  ).prefetch(4)\n  def on_epoch_end(self, batch, logs=None):\n    print(\"\\n\\nevaluated validation set\")\n    predict = self.model.predict(self.val_set)\n    predict = np.argmax(predict,-1)\n    print(\"predict done : \", predict.shape)\n    print(predict)\n    predictionstrings =[]\n    for i in range(len(self.instancedf)):\n      sample =self.instancedf.iloc[i]\n      offset = sample.offset \n      text = sample.text\n      res = refine(predict[i], offset)\n      for r in res:\n        predictionstrings.append(\n            {\n                \"predictionstring\":calc_word_indices(text, r['offset'][0],r['offset'][1]),\n                \"id\":sample.id,\n                \"class\":r['class']\n            }\n\n        )\n    oof=pd.DataFrame(predictionstrings)\n    f1s = []\n    if oof.shape[0] ==0:\n      print(\"predict none\")\n      return\n    CLASSES = oof['class'].unique()\n    for c in CLASSES:\n      pred_df = oof.loc[oof['class']==c].copy()\n      gt_df = self.val_df.loc[self.val_df['discourse_type']==c].copy()\n      f1 = score_feedback_comp(pred_df, gt_df)\n      print(c,f1)\n      f1s.append(f1)\n    print()\n    print('Overall',np.sum(f1s) / 7.)\n    print(\"to prediction strings\\n\\n\")\n\n\n","metadata":{"id":"qV2f7UK0mVHH","execution":{"iopub.status.busy":"2022-01-09T08:19:17.685158Z","iopub.execute_input":"2022-01-09T08:19:17.685524Z","iopub.status.idle":"2022-01-09T08:19:17.907431Z","shell.execute_reply.started":"2022-01-09T08:19:17.685488Z","shell.execute_reply":"2022-01-09T08:19:17.906707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig, TFAutoModel","metadata":{"id":"5TiVFbHd9T1g","execution":{"iopub.status.busy":"2022-01-09T08:19:17.910107Z","iopub.execute_input":"2022-01-09T08:19:17.910583Z","iopub.status.idle":"2022-01-09T08:19:17.922185Z","shell.execute_reply.started":"2022-01-09T08:19:17.910544Z","shell.execute_reply":"2022-01-09T08:19:17.921494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MAX_LENGTH","metadata":{"id":"zkx3LyJyiz7S","execution":{"iopub.status.busy":"2022-01-09T08:19:17.923616Z","iopub.execute_input":"2022-01-09T08:19:17.924147Z","iopub.status.idle":"2022-01-09T08:19:17.927582Z","shell.execute_reply.started":"2022-01-09T08:19:17.92411Z","shell.execute_reply":"2022-01-09T08:19:17.926776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def acc(y_true, y_pred):\n    mask = tf.where(y_true >= 0, 1, 0)\n   \n   \n    mask = tf.reshape(mask,(-1,1)) \n    mask = tf.cast(mask, tf.float32)\n    total = tf.math.reduce_sum(mask) + 0.00001\n    y_true  = tf.reshape(y_true,(-1,))\n    y_pred = tf.reshape(y_pred,(-1, 15))\n    y_pred = tf.math.argmax(y_pred, axis=-1)\n    y_pred = tf.reshape(y_pred,(-1,))\n    y_true = tf.cast(y_true,tf.int64)\n    l = tf.where(y_pred == y_true, 1., 0.)\n    l = tf.math.reduce_sum(l) / total\n    return l\n\n    # return tf.math.reduce_sum(m) / total\n\ndef custom_loss_function(y_true, y_pred):\n   mask = tf.where(y_true >= 0, 1, 0)\n   \n   \n   mask = tf.reshape(mask,(-1,1))\n   \n   mask = tf.cast(mask, tf.float32)\n   total = tf.math.reduce_sum(mask) + 0.0001\n   y_true  = tf.reshape(y_true,(-1,1))\n  #  mask = tf.where(y_true == 14, 0.5, mask)\n   y_true  = tf.where(y_true>=0 , y_true, 14)\n   y_pred = tf.reshape(y_pred,(-1, 15))\n  #  y_pred = tf.math.argmax(y_pred, -1)\n\n   scce = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.SUM)\n   lo = scce(y_true, y_pred,sample_weight=mask)\n   return tf.math.reduce_sum(lo)/total\n\n \n  #  squared_difference = tf.square(y_true - y_pred)\n  #  return tf.reduce_mean(squared_difference, axis=-1)\ndef mat_mul(a):\n  return tf.linalg.matmul(a[1],a[0], transpose_a=True)\ndef build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LENGTH,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LENGTH,), name = 'attention', dtype=tf.int32)\n    \n    mask = tf.keras.layers.Input(shape=(MAX_LENGTH, MAX_SENTENCE), name='sentence', dtype=tf.float32)\n\n    config = AutoConfig.from_pretrained(MODEL_NAME) \n    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n    x = backbone(tokens, attention_mask=attention)\n    x = tf.keras.layers.Lambda(mat_mul)([x[0],mask])\n    x = tf.keras.layers.Dense(256, activation='relu')(x)\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n    \n    \n\n    model = tf.keras.Model(inputs=[tokens,attention,mask], outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-4),\n                  loss = [custom_loss_function],\n                  metrics = [acc])\n    \n    return model","metadata":{"id":"5MVD73e-9fgY","execution":{"iopub.status.busy":"2022-01-09T08:19:17.929227Z","iopub.execute_input":"2022-01-09T08:19:17.929693Z","iopub.status.idle":"2022-01-09T08:19:17.946396Z","shell.execute_reply.started":"2022-01-09T08:19:17.929655Z","shell.execute_reply":"2022-01-09T08:19:17.945624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith strategy.scope():\n    model = build_model()","metadata":{"id":"8ZTl9E-q9ndw","outputId":"1a4308f7-bf67-4f34-a2e5-670dfab2a00b","execution":{"iopub.status.busy":"2022-01-09T08:19:17.947743Z","iopub.execute_input":"2022-01-09T08:19:17.948215Z","iopub.status.idle":"2022-01-09T08:20:04.058528Z","shell.execute_reply.started":"2022-01-09T08:19:17.94818Z","shell.execute_reply":"2022-01-09T08:20:04.057772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(\"../input/sentence-level/best_05.hdf5\")\nmodel.summary()","metadata":{"id":"YTrLtjO4-4Dk","outputId":"ecef8a11-6a12-4132-c95b-4686ee9a09f7","execution":{"iopub.status.busy":"2022-01-09T08:20:04.059774Z","iopub.execute_input":"2022-01-09T08:20:04.061135Z","iopub.status.idle":"2022-01-09T08:20:04.089778Z","shell.execute_reply.started":"2022-01-09T08:20:04.061089Z","shell.execute_reply":"2022-01-09T08:20:04.089085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df.iloc[valid_idx]","metadata":{"id":"0DfIWrWiwNbt","execution":{"iopub.status.busy":"2022-01-09T08:20:04.09091Z","iopub.execute_input":"2022-01-09T08:20:04.091184Z","iopub.status.idle":"2022-01-09T08:20:04.09585Z","shell.execute_reply.started":"2022-01-09T08:20:04.091148Z","shell.execute_reply":"2022-01-09T08:20:04.094864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"iuKBWr2DdnQM","execution":{"iopub.status.busy":"2022-01-09T08:20:04.097456Z","iopub.execute_input":"2022-01-09T08:20:04.098049Z","iopub.status.idle":"2022-01-09T08:20:04.103587Z","shell.execute_reply.started":"2022-01-09T08:20:04.09801Z","shell.execute_reply":"2022-01-09T08:20:04.102799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# valid_pd","metadata":{"id":"G5HPzBhqyu1R","execution":{"iopub.status.busy":"2022-01-09T08:20:04.105232Z","iopub.execute_input":"2022-01-09T08:20:04.105515Z","iopub.status.idle":"2022-01-09T08:20:04.112603Z","shell.execute_reply.started":"2022-01-09T08:20:04.105478Z","shell.execute_reply":"2022-01-09T08:20:04.111882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bs,1024,768\n# bs,1024,150\n\n# RNING RATE SCHEDULE AND MODEL CHECKPOINT\nEPOCHS = 5\nBATCH_SIZE = 4 \nLRS = [1e-5, 0.5*1e-5, 0.25*1e-5, 0.25*1e-5, 0.25e-5, 0.25e-5, 0.25e-5, 0.1e-5] \ndef lrfn(epoch):\n    return LRS[epoch]\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\niou_cb  = IOUCallBack(model, valid_pd)\ncheckpoint_filepath = './best_{epoch:02d}.hdf5'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=False)","metadata":{"id":"xEPDfwiNCEnw","execution":{"iopub.status.busy":"2022-01-09T08:20:04.114144Z","iopub.execute_input":"2022-01-09T08:20:04.114719Z","iopub.status.idle":"2022-01-09T08:20:04.152894Z","shell.execute_reply.started":"2022-01-09T08:20:04.11468Z","shell.execute_reply":"2022-01-09T08:20:04.152195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# iou_cb.on_epoch_end(1)","metadata":{"id":"ESHYCUvPdkbI","execution":{"iopub.status.busy":"2022-01-09T08:20:04.154224Z","iopub.execute_input":"2022-01-09T08:20:04.154476Z","iopub.status.idle":"2022-01-09T08:20:04.157975Z","shell.execute_reply.started":"2022-01-09T08:20:04.154443Z","shell.execute_reply":"2022-01-09T08:20:04.157193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_set,\n              # validation_data = val_set,\n              callbacks = [lr_callback, model_checkpoint_callback,iou_cb],\n              epochs = 4\n          )","metadata":{"id":"vSkP9AeCcvmv","outputId":"d4907219-5ab1-4226-bdf2-f9395b8e8ef9","execution":{"iopub.status.busy":"2022-01-09T08:20:04.159228Z","iopub.execute_input":"2022-01-09T08:20:04.159974Z","iopub.status.idle":"2022-01-09T08:21:12.971325Z","shell.execute_reply.started":"2022-01-09T08:20:04.159924Z","shell.execute_reply":"2022-01-09T08:21:12.969827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.fit(train_set,\n#           validation_data = val_set,\n#           callbacks = [lr_callback, model_checkpoint_callback,iou_cb],\n#           epochs = 1\n#           )","metadata":{"id":"KsQxpx51kMkt","execution":{"iopub.status.busy":"2022-01-04T07:58:05.01528Z","iopub.status.idle":"2022-01-04T07:58:05.015805Z","shell.execute_reply.started":"2022-01-04T07:58:05.015585Z","shell.execute_reply":"2022-01-04T07:58:05.015609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"hSZuS20XI5O_"},"execution_count":null,"outputs":[]}]}